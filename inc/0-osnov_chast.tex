\section{Алгоритмы Генерации Доменных Имен}\label{dga}
Алгоритмы Генерации Доменных Имен (DGA) представляют собой алгоритмы, используемые вредоносным программным обеспечением (malware) для генерации большого количества псевдослучайных доменных имен, которые позволят им установить соединение с управляющим командным центром. Тем самым они обеспечивают мощный слой защиты инфраструктуры для вредоносных программ. С первого взгляда концепция создания большого количества доменных имен для установки связи кажется не сложной, но методы, используемые для создания произвольных строк часто скрываются за разными слоями обфускации. Это делается для усложнения процесса обратной разработки и получения модели функционирования того или иного семейства алгоритмов. В разделе \ref{work_princip} рассмотрены общие принципы алгоритмов генерации доменных имен, а раздел \ref{dga_teor} раскрывает особенности реализаций некоторых из рассмотренных алгоритмов.

    \subsection{Общие принципы работы}\label{work_princip}
        Общий принцип работы представлен на рис \ref{dga_work}. В общем случае вредоносному файлу необходим какой-либо параметр для инициализации Генератора Псевдослучайных Чисел (ГСПЧ). В качестве этого параметра может выступать любой параметр, который будет известен вредоносному файлу и владельцу ботнета. В нашем случае - это значение текущей даты и времени. Вредоносный файл, используя протокол HTTP посылает запрос на сайт cnn.com. В ответ на этот запрос cnn.com возвращает в заголовках HTTP ответа текущие время и дату в формате GMT. Владелец ботнета таким же способом получает текущее время и дату в формате GMT. Далее, это значение, попадает в сам алгоритма генерации доменных имен, инициализируя ГСПЧ, который может иметь вид например Линейного конгруэнтного генератора или  Регистра сдвига с обратной связью. Поэтому, используя одинаковые вектора инициализации, вредоносный файл и владелец ботнета получают идентичные таблицы доменных имен.
        После этого владельцу ботнета достаточно зарегистрировать лишь один домен, для того, чтобы вредоносный файл, рекурсивно посылая запросы к DNS серверу получил IP адрес управляющего сервера для дальнейшей установки с ним соединения и получения, выполнения команд.
        \addimghere{images/dga_work}{0.8}{Общий принцип работы}{dga_work}

    \subsection{Рассмотренные алгоритмы}\label{dga_teor}
    В ходе выполнения данной работы были проанализированы 8 разновидностей алгоритмов генерации доменных имен, а именно: Conficker, Cryptolocker, Ramdo, PushDo, Zeus, Tinba, Rovnix, Matsnu. Для каждого из них, путём обратной разработки были составлены модели работы алгоритмов генерации доменных имен и реализованы на языках программирования высокого уровня. Далее в работе представлены описание и особенности работы каждого из этих алгоритмов.

Conficker - вирус, впервые появившийся в 2008 году, использующий для заражения машин популярную уязвимость MS08-067. Одним из первых применил технику DGA. Процесс генерации доменного имени можно описать пятью шагами, как показано на рис \ref{conficker_dga} и его архитектура идентична принципу, описанному в части \ref{work_princip}.
\addimghere{images/conficker}{0.5}{Принцип работы conficker DGA}{conficker_dga}

Cryptolocker - имя вредоносных программ вида троян-вымогатель (ransomware). Целью данного вируса являются системы Microsoft Windows. Cryptolocker полностью шифрует содержимое файловой системы жертвы и требует заплатить выкуп для получения ключа дешифрования.
DGA, который использует cryptolocker относительно прост, однако использует множество приемов, которые осложняют процесс его обратной разработки. Его алгоритм использует для инициализации 4 значения - ключ, день, месяц, год.
Ключ может быть константой или рассчитываться по формуле
\begin{equation}
key = ((key * 0x10624DD3) >> 6) * 0xFFFFFC18)+ key
\end{equation}
В данной работе за значение ключа взята константа 0x41.
Каждый символ рассчитывается по формуле
\begin{equation}
chr(ord(a) + (year \; xor \; month\; xor \;date) \;\% \;25)
\end{equation}
А длина составляет доменного имени составляет
\begin{equation}
date>>3 \; xor \; year>>8 \; xor  \;year>>11 \; and \; 3 + 12
\end{equation}
В финальной стадии генерации доменного имени добавляется домен верхнего уровня, который последовательно выбирается из массива зашитых значений.

PushDo - второй по величине ботнет, впервые появившийся в 2007 году. Для установки связи с командным сервером использует два механизма. Первый - зашитые доменные имена, и второй - DGA. Второй механизм имеет четыре составляющие, а именно - системное время, вектор инициализации, функцию хеширования MD5 и сам генератор доменных имен. Каждый день, при генерации первого доменного имени DGA вектору инициализации присваивается значение системного времени, которое получается путем вызова WINAPI функции GetLocalTime, операций битового сдвига и XOR - функция шифрования. После этого вектор инициализации попадает на вход функции хеширования MD5, вывод которой в свою очередь подает в генератор доменного имени и вектор инициализации генерации следующего доменного имени. Однако вектор инициализации следующего доменного имени берет не весь результат функции MD5, а лишь первые её 4 байта. Схема работы этого алгоритма генерации доменных имен представлена на рис \ref{PushDo}.
\addimghere{images/PushDo}{0.8}{Принцип работы PushDo DGA}{PushDo}
Отдельно стоит рассмотреть генератор доменного имени, получающий на вход результат функции MD5. Каждый экземпляр такого генератора имеет четыре уникальных строки. Это позволяет обновлять генератор с каждой новой версий ботнета или иметь множество непересекающихся ботнетов одного и того же типа. Сначала генератор вычисляет длину доменного имени. Она рассчитывается делением первых четырех байт на 4 и лежит в пределах от 9 до 12. Далее специальный цикл, используя остаток от деления переводит вывод функции MD5 в читаемое доменное имя и добавляет к нему домен верхнего уровня. В итоге, всего, каждый день PushDo DGA генерирует 30 доменных домен, зависящих не только от системного времени, но и от зашитых значений - специальных строк генератора.

Zeus - семейство вредоносных программ, ворующих аутентифицирующие данные, которое впервые появилось в 2007 году. Первые два варианта этого ботнета базировались на зашитых централизованных адресах управляющих серверов. Эти сервера постоянно отслеживались и отключались при помощи антивирусных компаний и центров реагирования на инциденты информационной безопасности. Именно поэтому, в 2011 году было обнаружено появление новой версии этого ботнета, который использует одноранговые сети и DGA для защиты своей инфраструктуры. В первую очередь защита построена на одноранговых сетях, однако если все зараженные машины не отвечают, то используется DGA. Схема его реализация схожа с алгоритмом, используемом в ботнете PushDo. Алгоритм также использует системное время, алгоритм хеширования MD5 и генератор доменного имени. Однако в отличии от PushDo системное время является вектором инициализации для всех 1000 доменных имен генерируемых каждую неделю, а номер доменного имени является параметром salt (соль) в функции хеширования MD5. Генератор доменного имени можно представить в виде следующего псевдокода.
\begin{lstlisting}
hash = MD5(time+salt)
name = ""
for (j = 0; j < len(hash); j++) {
    c1 = (hash[j] & 0x1F) + ’a’;
    c2 = (hash[j] / 8) + ’a’;
    if(c1 != c2 && c1 <= ’z’) name += c1;
    if(c1 != c2 && c2 <= ’z’) name += c2;
}
\end{lstlisting}

Семейство Ramdo, впервые обнаруженное в декабре 2013 года кардинально отличается от других ботнетов. Ramdo используют множество приемов антиотладки, а также использует технологию double flux для установки соединения с управляющими серверами. Несмотря на сложность обратной разработки данный ботнет имеет достаточно простой и небольшой DGA, который основывается на зашитой константе и итераторе, основанного на регистре сдвига и булевой операции исключающее или. Для формализации модели DGA данного семейства в работе использован исходный код из работы [15]. Кроме этого идентичную модель DGA использует и Tinba, более известный как Tiny Banker.  Основное отличие лишь в том, что для генерации следующего доменного имени Tinba использует также зашитый первоначальный адрес управляющего сервера.

Бурное развитие методов идентификации и фильтрации вредоносных доменных имен привело к развитию новых видов DGA. Такими примерами являются алгоритмы генерации доменных имен ботнетов Rovnix и Matsnu. Их алгоритмы проектировались специально для обхода систем идентификации, построенных на количественной оценки энтропии доменного имени или лингвистических моделях, например n-gramm. Rovnix первым из ботнетов предложил использовать специально заготовленный текст для генерации доменного имени. В качестве такого текста Rovnix использует Декларацию о независимости США. На основе системного времени Rovnix выбирает слова из данного текста и конкатенирует их, для достижения определенной длины. Ботнет Matsnu имеет схожий подход, вредоносная программа использует два зашитых словаря, один из которых содержит существительные, а второй глаголы. Тем самым, основываясь на системном времени алгоритм генерации доменного имени выбирает существительное, а затем выбирает глаголы, для достижения необходимой длины доменного имени. Стоит отметить, что подходы этих двух ботнетов действительно с легкостью обходят системы идентификации, основанные лишь на энтропийной оценке, поэтому для идентификации этих классов вредоносных доменных имен требуются более сложные модели, описанные в главах \ref{be_class_exp} и \ref{lstm_class_exp}.
\clearpage

\section{Существующие подходы к классификации}\label{be_class}
В данной главе рассматриваются теоретические основы существующих подходов классификации вредоносных доменных имен. Их реализация и результаты их работы представлены в разделе \ref{be_class_exp}.

В настоящее время существует множество работ, связанных анализом алгоритмов генерации доменных имен. Проблемы автоматического анализа алгоритмов DGA и пути их решения можно найти в статье [1]. Идея использования методов машинного обучения освещена в работе [7]. Так, ряд известных компаний, занимающихся информационной безопасностью (Damballa, OpenDns, Click Security и др.), применяют подобные решения для анализа и фильтрации сетевой активности вредоносных программ. Например, Click Security в своей работе [3] предлагают использовать решающие деревья для бинарной классификации на принадлежность доменов к вредоносным. Для этого ими предложен способ выделения признаков из домена. Стоит отметить работу [4], которая рассматривает возможность классификации, используя метод опорных векторов (Support Vector Machine) и выделения из доменов признаков n-gram - подстрока, состоящая из последовательных n символов исходной строки. Схожий подход описывается и в работе [5]. Однако, в отличие от работы [4], имеет большую практическую направленность и предлагает использование алгоритма C4.5 (алгоритм для построения деревьев решений). Подход, основанный на анализе морфем в статье [8], является неактуальным, так как последние исследования [6] показывают, что алгоритмы генерации доменных имен совершенствуются с целью обхода существующих способов обнаружения. В результате анализа существующих работ выделены 5 наиболее перспективных существующих подходов к классификации, представленные в разделах \ref{bayes}, \ref{log_reg}, \ref{random_forest}, \ref{extra_tree}, \ref{voting}.

    \subsection{Naive Bayes}\label{bayes}
    \subsection{Logistic Regression}\label{log_reg}
    \subsection{Random Forest}\label{random_forest}
    \subsection{Extra Tree Forest}\label{extra_tree}
    \subsection{Voting Classification}\label{voting}
\clearpage

\section{Нейронные сети}\label{ner_nextwork}
    Искуственные нейронные сети первоначально были разработаны как математические модели, отображающие деятельность мозга. Базовая структура нейронной сети представляет собой сеть узлов, соедниненных взвешенными связями. Узлы представляют собой нейроны, а взвешенные связи - силу этой связи. Нейронная сеть активируются путем подачи информации в нейроны, и эта активация распространяется по всей сети вдоль взвешенных связей. Впервые такую сеть описали У. Маккалок и У. Питтс в 1943 году. В настоящее время существует множество разновидностей таких сетей с разнообразными свойствами. Например, важное отличие между ними является наличие цикла, так например сети, не имеющие циклов называются сети прямого распространения (feedforward neural networks), а сети, имеющие цикл - рекурсивными или рекуррентными нейронными сетями. Наиболее распространенной сетья прямого распространения является многослойный перцептрон Румельхарта. Данная архитектура нейронной сети имеет входной слой, скрытые слоя и выходной слой. Процесс прохода этих слоев называется прямым (forward pass). Обучая такую модель мы изменяем веса связей, и именно они определяют функцию зависимости выходного вектора от входного. Хорник в 1989 году доказал, что многослойный перцептрон с одним скрытым слоем, содержащий достаточной количество нелинейных cвязей с опредленной точностью способен апроксимировать любую непрерывную функцию. Исходя из этого, многослойный перцепртрон часто используется для апроксимации какого-либо закона природы.
    
    Рассмотрим многослойный перцептрон с $I$ входными нейронами, которые активируются входным вектором $x$. Каждый нейрон первого скрытого слоя вычисляет взвешенную сумму входных нейронов. Для скрытого нейрона $h$ мы обозначим это как $a_{h}$. Далее $a_{h}$ подаётся в функцию активации $\theta_{h}$. Обозначив веса от нейрона $i$ к нейрону $j$ как $\omega_{ij}$ имеем
    \begin{equation}
    a_{h}=\sum_{i=1}^{I}w_{ij}x_{i}
    \end{equation}
    \begin{equation}
    b_{h} = \theta _{h}(a_{h}})
    \end{equation}
    Таким образом каждый слой такой нейронной сети состоит из сумматора и функции активации. В качестве функции активации могут выступать различные функции, но наиболее часто используемые - сигмоид и гиперболический тангенс.
    \begin{equation}
    \sigma(x) = \frac{1}{1+e^{-x}}
    \end{equation}
    \begin{equation}
    tanh(x) = \frac{e^{2x}-1}{e^{2x}+1}
    \end{equation}
    Одно из свойств сигмоида позволяет усиливать слабые сигналы, не завися от входных слишком сильных сигналов. В отличии от гиперболического тангенса область значений сигмоида лежит между 0 и 1, в то время как гиперболический тангенс принимает значения от -1 до 1. Важное свойство этих двух функций активации - это их нелинейность. Это позволяет нейронной сети апроксимировать любые нелинейные функции. Кроме этого, это свойство позволяет строить нейронные сети с несколькими скрытыми слоями, в отличии от нейронных сетей с линейной функцей активацией, где любая такая нейронная сеть с несколькими скрытыми слоями эквивалентна нейронной сети с одним линейным скрытым слоем. Это позволяет создавать более эффективные модели.
    Другое важное свойство этих функции заключается в их дифференцируемости. Это позволяет обучать нейронную сеть при промощи алгоритма градиентного спуска. А то, что функции могуть быть легко выражены через самих себя существенно сокращает вычислительную сложность метода обратного распространения ошибки.
    \begin{equation}
    \frac{\partial \sigma (x)}{\partial x} = \sigma (x)(1 - \sigma (x))
    \end{equation}
    \begin{equation}
    \frac{\partial tanh(x)}{\partial x} = 1 - tanh(x)^2
    \end{equation}
    После расчета блока в первом скрытом слое, значения передаются в следующий слой, где опять проходят процесс суммирования и подает в функции активации до тех пор, пока не попадут в выходной слой. Этот вектор $y$ также попадает в нейрон выходного слоя, происходит суммирование
    \begin{equation}
    a_{k}=\sum_{h\in  H_{i}}w_{hk}x_{h}
    \end{equation}
    Количество выходных нейронов и выходной функции активации зависит от поставленной задачи. Для задач бинарной классификации обычно используется сигмоид. Выходное значение $y$ этой функции можно трактовать как вероятность принадлежности к первому классу, а значение $1-y$ как вероятность принадлежности ко второму классу.
    Для задачи многоклассовой классификации можно использовать $softmax$ функцию
    \begin{equation}
    p(C_{k}|x)=\frac{e^{a_{k}}}{\sum_{k{}'=1}^{K}e^{a_{k{}'}}}
    \end{equation}
    В результате на выходе мы имеем K выходных значений, которые мы можем интерпретировать как вероятности принадлежности к конкретному классу. Поэтому многослойный перцептррон так часто используется для задач классификации. Поскольку все операции описанные выше дифференцируемы, многослойный перцептрон може т быть обучен при помощи метода градиентного спуска. Основная идея этого метода найти производную функцию потерь относительно каждого из весов сети, и изменить их в направлении наиболее быстрого убывания функции потерь, для того, чтобы увеличить вероятность выбора правильного класса при задаче классификации. Пусть $z$ - правильно выбранный класс, тогда функцию потерь можно представить в виде
    \begin{equation}
    \mathcal{L}(x,z) = (z-1)ln(1-y)-zlny
    \end{equation}
    Для эффективного расчета градиента в этой работе используется алгоритм обратного распространения ошибки, описанный в работе [17]. Однако нейронные сети без циклов имеют существенный недостаток - выход такой сети зависит лишь от входных векторов в конкретный момент времени и не зависит от предыдущих или будущих входных векторов, а лишь подстраивает веса целевой функции в процессе обучения. Для решения этой проблемы применяются рекуррентные нейронные сети.
    Рекуррентные нейронные сети главным образом отличаются наличием цикла. Это позволяет не только сопоставить входному векторы выходной, но и иметь зависимость выходного вектора от всех предыдущих входных векторов.На рис \ref{RNN} изображена развертка такой сети. На каждой итерации в нейронную сеть подается на вход вектор $x_{t}$, а на выходе вектор скрытых состояний $h_{t}$.
    \addimghere{images/rnn/RNN}{0.8}{Рекуррентная нейронная сеть}{RNN}
    Обучение такой нейронной сети схоже с обучением многослойного перцептрона и в нашем случае основывается на алгоритме обратного распространения ошибки. Исходя из вышесказанного, мы можем утверждать, что такая архитектура нейронной сети может анализировать информацию, поданную ранее для анализа информации в настоящий момент времени. Однако, на практике оказывается, что если разрыв между прошлой информацией и настоящей достаточно велик, то эта связь теряется и такая сеть неспособна её обрабатывать. Решение этой проблемы было найдено в 1997 году учеными Hochreiter & Schmidhuber. В своей работе они предложили новую модель рекуррентной нейронной сети, а именно Long short-therm memory.
    \subsection{Long short-term memory}
    Long short-therm memory (LSTM) - модель рекуррентной нейронной сети, способная к обучению долгосрочных зависимостей. В настоящее время данная модель широко используется для всемозможных классов задач, таких как: распознование речи, обработка естественных языков и др. LSTM состоит из ряда постоянно связанных подсетей, известных как блоки памяти. Вместо одного слоя нейронной сети, в данной модели используется 4 слоя, взаимодействующих особым образом. Главное в модели LSTM – это ячейка памяти. Модель имеет возможность удалить или добавить информацию в память, и это процесс управляется структурами, которые называют gates. Всего имеется 3 таких структуры: input gate, forget gate, output gate.
    
    На первом шаге LSTM мы решаем какую иформацию мы хотим выбросить из ячейки памяти $C_{t}$. Для этого вектор $h_{t-1}$ попадает в forget gate
    \begin{equation}\label{fg:1}
    f_{t} = \sigma(W_{f}[h_{t-1,x_{t}}]+b_{f})
    \end{equation}
    проходя через сигмоиду на выходе мы получаем значение от 0 до 1. 1 означает ``забыть все'', в то время как 0 означает ``оставить все''.
    На следующем шаге мы определяем какую информацию мы хотим добавить в ячеку памяти $C_{t}$. Для этого выполняется 2 операции. Во-первых, input gate решает какую информацию мы обновляем \ref{in:1}. Во-вторых, слой с функцией активации гиперболический тангенс создает вектор значений $\tilde{C_{t}}$, который будет добавлен в $C_{t}$ \ref{in:2}.
    \begin{equation}\label{in:1}
    i_{t}=\sigma(W_{i}[h_{t-1},x_{t}]+b_{i})
    \end{equation}
    \begin{equation}\label{in:2}
    \tilde{C_{t}} = tanh(W_{C}[h_{t-1},x_{t}]+b_{C})
    \end{equation}
    Теперь, использую информацию, полученную на предыдущих шагах \ref{fg:1} \ref{in:1} \ref{in:2}, мы можем обновить ячейку памяти $C_{t-1}$, получив $C_{t}$. Для этого мы умножим $f_{t}$ на $C_{t-1}$, чтобы ``забыть'' старую информацию и добавим $i_{t}*\tilde{C_{t}}$, чтобы обновить $C_{t-1}$. В итоге получим
    \begin{equation}\label{update}
    С_{t}=f_{t}*C_{t-1}+i_{t}*\tilde{C_{t}}
    \end{equation}
    В завершении, вектор $h_{t-1}$ попадает в output gate, проходя через функцию активации сигмоид \ref{output}, а новый вектор $h_{t}$ получается путем перемножения выхода output gate со значением $tanh(C_{t})$ \ref{output:2}.
    \begin{equation}\label{output}
    o_{t}=\sigma(W_{o}[h_{t-1},x_{t}]+b_{o})
    \end{equation}
    \begin{equation}\label{output:2}
    h_{t}=o_{t}*tanh(C_{t})
    \end{equation}
    Таким образом, автором описана стандартная структура модели LSTM. Она имеет множество вариаций и дополнейний. В данной работе, в разделе \ref{lstm_class_exp} использована одна из распространненых таких вариаций, а именно Gated Recurrent Unit. Данная модель отличается тем, что forget gate и input gate объединены в один update gate. Данная модель описывается следующими формулами \ref{gr:1}, \ref{gr:2}, \ref{gr:3}, \ref{gr:4}.
    \begin{equation}\label{gr:1}
    z_{t}=\sigma(W_{z}[h_{t-1},x_{t}])
    \end{equation}
    \begin{equation}\label{gr:2}
    r_{t}=\sigma(W_{r}[h_{t-1},x_{t}])
    \end{equation}
    \begin{equation}\label{gr:3}
    \tilde{h_{t}}=tanh(W[r_{t}*h_{t-1,x_{t}}])
    \end{equation}
    \begin{equation}\label{gr:4}
    h_{t}=(1-z_{t})*h_{t-1}+z_{t}*\tilde{h_{t}}
    \end{equation}
\clearpage

\section{Эксперимент}\label{experiment}
    Для тестирования моделей была составлена обучающая выборка. Выборка состоит из 2 классов. Первый - Legit, был взят из списка Alexa Top Million. Второй - DGA, был составлен путем обратной разработки алгоритмов генерации вредоносных доменных имен, взятых из экземпляров вредоносных программ, существующих в сети Интернет. Данный процесс описан в разделе \ref{dga_teor}. Получившийся результат приведен в таблице 
    \begin{table}[!ht]
    \centering
    \caption{Обучающая выборка}\label{}
    \begin{tabular}{@{}cc@{}}
    \toprule
    Имя          & Количество \\ \midrule
    Legit        & 1000000    \\
    Cryptolocker & 100000     \\
    Zeus         & 100000     \\
    Pushdo       & 100000     \\
    Rovnix       & 100000     \\
    Tinba        & 100000     \\
    Matsnu       & 100000     \\
    Ramdo        & 100000     \\ \bottomrule
    \end{tabular}
    \end{table}
    \subsection{Существующие подходы}\label{be_class_exp}
    Не секрет, что зачастую самым важным при решении задачи является умение правильно отобрать и даже создать признаки. В англоязычной литературе это называется Feature Selection и Feature Engineering. Автор предлагает использовать следующий список параметров: length, entropy, alexa gram, word gram, diff.
    Первым параметром выступает длина доменного имени.
    Второй параметр - энтропия рассчитывалась по формуле \ref{entropy_equation}.
    \begin{equation}\label{entropy_equation}
    H = \sum_{i=1}^{n} p(i)*log_{2}\;p(i{})
    \end{equation}
    Далее, была рассмотрена модель $N-gram$. Каждый $n-gram$ (от 3 до 5) был представлен как вектор в n-мерном пространстве, и расстояние между ними было рассчитано с помощью скалярного произведения этих векторов. Более подробно этот метод изложен в работе [11]. В результате мы получили 2 параметра :
    - косинусное расстояние до словаря, состоящего из доменов Alexa Top Million.
    - косинусное расстояние до специально составленного словаря, состоящего из наиболее употребительных слов и фраз.
    Проанализировав существующие параметры, авторами было решено добавить в модель параметр diff:
    \begin{equation}
    diff = alexa\;gram-word\;gram
    \end{equation}
    Для анализа каждого из предложенных параметров были построены наглядные графики.
    \addimghere{images/linear_class/length.png}{0.6}{Длина доменных имен}{length}
    \addimghere{images/linear_class/entropy.png}{0.6}{Энтропия доменных имен}{entropy}
    \addimghere{images/linear_class/entropy2.png}{0.6}{Зависимость энтропии доменных имен от длины доменных имен}{entropy2}
    \addimghere{images/linear_class/alexa_gramm.png}{0.6}{Зависимость параметра alexa gram от длины доменного имени}{alexa_gramm}
    \addimghere{images/linear_class/word_gramm.png}{0.6}{Зависимость параметра word gram от длины доменных имен}{word_gramm}
    \addimghere{images/linear_class/diff.png}{0.6}{Зависимость параметра diff от длины доменных имен}{diff}
    Классификация проводилась по принципу 80/20, т.е. обучение проводилось на 80\% исходных данных, а тестирование алгоритма на оставшихся 20\%.В качестве алгоритмов классификации использовались такие алгоритмы как:
    \begin{itemize}
    \item Logistic Regression
    \item Random Forest
    \item Naive Bayes
    \item Extra Tree Forest
    \item Voting Classification
    \end{itemize}
    После тестировании качества классификации были получены следующие результаты.
    \begin{table}[!ht]
    \centering
    \caption{Качество классификации}\label{}
    \begin{tabular}{@{}ccc@{}}
    \toprule
    Алгоритм              & Бинарная & Многоклассовая \\ \midrule
    Logistic Regression   & 86,7\%   & 78,8\%         \\
    Random Forest         & 95\%     & 89,3\%         \\
    Naive Bayes           & 75,2\%   & 75,6\%         \\
    Extra Tree Forest     & 94,6\%   & 88,9\%         \\
    Voting Classification & 94,7\%   & 90\%           \\ \bottomrule
    \end{tabular}
    \end{table}
    Мы проводили классификацию не только на вредоносные и легитимные домены, но и по принадлежности к определенному типу DGA. В многоклассовой классификации наилучший результат показал алгоритм Voting Classification. Данный алгоритм реализован в библиотеке Scikit-learn для языка программирования Python и представляет собой классификатор на основе результатов других статистических оценок. При распозновании вредоносных доменов лучше всего себя показал алгоритм Random Forest . Для наглядности количественной оценки модели была построена матрица неточностей (Сonfusion matrix), которая содержит правильно распознанные хорошие примеры (True Positive), правильно распознанные плохие примеры (True Negative) и ошибки в распозновании (False Positive, False Negative).
    \begin{table}[!ht]
    \centering
    \caption{Матрица неточностей}\label{}
    \begin{tabular}{@{}cc@{}}
    \toprule
    Имя         & Точность \\ \midrule
    Legit/Legit & 95\%     \\
    Legit/DGA   & 5\%      \\
    DGA/Legit   & 4,3\%    \\
    DGA/DGA     & 95,7\%   \\ \bottomrule
    \end{tabular}
    \end{table}
    Наибольший интерес представляет показатель False Positive, так как именно он по нашему мнению определяет удобство применение данной модели в системе мониторинга инцидентов информационной безопасности. 
    \subsection{Long short-term memory}\label{lstm_class_exp}
    \subsection{Сравнительный анализ}\label{compare}
\clearpage